{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# autoreload\n",
            "%load_ext autoreload\n",
            "%autoreload 2"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import os\n",
            "\n",
            "if os.getcwd().split(\"/\")[-1] != \"Road-Segmentation-ML\":\n",
            "    os.chdir(\"..\")\n",
            "print(\"CWD:\", os.getcwd())"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import torch\n",
            "from omegaconf import OmegaConf\n",
            "import segmentation_models_pytorch as smp\n",
            "from train_utils import prepare_data, prepare_model, prepare_optimizer, train, set_seeds"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [],
         "source": [
            "# set seeds\n",
            "set_seeds()\n",
            "\n",
            "# create folder models if it doesn't exist\n",
            "if not os.path.exists(\"models\"):\n",
            "    os.makedirs(\"models\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "args = OmegaConf.create(\n",
            "    dict(\n",
            "        # General\n",
            "        seed=0,\n",
            "        # Data folders\n",
            "        image_folders=[\"datasets/train/images/\"],\n",
            "        gt_folders=[\"datasets/train/groundtruth/\"],\n",
            "        # Data tranforms\n",
            "        # random_resized_crop: crop a random portion of image and resize it to a given size\n",
            "        random_resized_crop=False,\n",
            "        output_size=(400, 400),  # expected output size of the crop, for each edge.\n",
            "        input_size=(384),  # resize\n",
            "        random_resized_crop_scale=(0.5, 0.5),\n",
            "        # random_horizontal_flip: randomly flip the image horizontally with a given probability\n",
            "        random_horizontal_flip=True,\n",
            "        # random_vertical_flip: randomly flip the image vertically with a given probability\n",
            "        random_vertical_flip=True,\n",
            "        # random_rotation: randomly rotate the image with a given probability\n",
            "        random_rotation=False,\n",
            "        degrees=5,  # range of degrees to select from\n",
            "        # color_jitter: randomly change the brightness, contrast and saturation of an image\n",
            "        color_jitter=False,\n",
            "        brightness=0.1,  # how much to jitter brightness.\n",
            "        contrast=0.1,  # how much to jitter contrast.\n",
            "        saturation=0.1,  # how much to jitter saturation.\n",
            "        hue=0.1,  # how much to jitter hue.\n",
            "        # normalization\n",
            "        normalization=True,  # TODO: Should it always be True?\n",
            "        # Data loaders\n",
            "        batch_size=2,\n",
            "        train_size=0.8,\n",
            "        val_size=0.2,\n",
            "        # Model\n",
            "        # UNetV1\n",
            "        model_name=\"UNetV3\",  # Change for v3\n",
            "        model_save_name=\"models/checkpoints/unetv3-test.pt\",\n",
            "        # Optimizer\n",
            "        optim_name=\"adam\",  # sgd\n",
            "        optim_lr=0.001,\n",
            "        optim_momentum=0.9,  # TODO: Try with (optim_momentum != 0) and without (momentum = 0)?\n",
            "        # Training\n",
            "        n_steps=2000,\n",
            "        eval_freq=100,\n",
            "        # Wandb logging\n",
            "        wandb_project=\"road-segmentation\",\n",
            "        wandb_run=\"unet-v3\",\n",
            "        entity=\"feeit\",\n",
            "    )\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# prepare train and validation loaders\n",
            "train_loader, val_loader = prepare_data(args)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# prepare model\n",
            "model = prepare_model(args)\n",
            "# define loss function\n",
            "criterion = torch.nn.BCEWithLogitsLoss()\n",
            "# criterion = DiceLoss()\n",
            "# prepare optimizer\n",
            "optimizer = prepare_optimizer(model, args)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# device to use for training\n",
            "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "model.to(device)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# train\n",
            "trained_model = train(\n",
            "    model, device, train_loader, val_loader, criterion, optimizer, args\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# from train_utils import batch_mean_and_sd\n",
            "\n",
            "# # calculate mean and std of the dataset\n",
            "# mean, std = batch_mean_and_sd(train_loader)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Submission"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "metadata": {},
         "outputs": [],
         "source": [
            "import torch\n",
            "import torchvision.transforms.v2 as transforms\n",
            "import torchvision.models.segmentation as models\n",
            "import matplotlib.pyplot as plt\n",
            "from datasets.TestDataset import TestDataset\n",
            "from models.UNetV3 import UNetV3\n",
            "from examples.mask_to_submission import *"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Loading checkpoint failed. Trying to load it with map_location.\n"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "<All keys matched successfully>"
                  ]
               },
               "execution_count": 7,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "# select checkpoint\n",
            "MODEL = \"models/checkpoints/unetv3-3_effnet.pt\"\n",
            "# load checkpoint\n",
            "try:\n",
            "    checkpoint = torch.load(MODEL)\n",
            "except:\n",
            "    print(\"Loading checkpoint failed. Trying to load it with map_location.\")\n",
            "    checkpoint = torch.load(MODEL, map_location=torch.device(\"cpu\"))\n",
            "# create model\n",
            "model = UNetV3()\n",
            "# load model weights\n",
            "model.load_state_dict(checkpoint)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "/home/nadezhda/.local/lib/python3.10/site-packages/torchvision/transforms/v2/_deprecated.py:43: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
                  "  warnings.warn(\n"
               ]
            }
         ],
         "source": [
            "# path to the test folder\n",
            "test_folder = \"datasets/test/\"\n",
            "\n",
            "# set mean and std\n",
            "std = [0.1967, 0.1896, 0.1897]\n",
            "means = [0.3353, 0.3328, 0.2984]\n",
            "\n",
            "# define transformations\n",
            "transform = transforms.Compose(\n",
            "    [\n",
            "        transforms.ToTensor(),\n",
            "        transforms.Normalize(means, std=std),\n",
            "    ]\n",
            ")\n",
            "\n",
            "# create test dataset\n",
            "test_dataset = TestDataset(test_folder, transform=transform)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# create folder predictions if it doesn't exist\n",
            "if not os.path.exists(\"predictions\"):\n",
            "    os.makedirs(\"predictions\")\n",
            "# save predictions\n",
            "prediction_filenames = []\n",
            "for i in range(len(test_dataset)):\n",
            "    # get image\n",
            "    image = test_dataset[i]\n",
            "    # create prediction\n",
            "    prediction = model(image.unsqueeze(0))\n",
            "    # threshold prediction\n",
            "    prediction = torch.sigmoid(prediction)\n",
            "    prediction = (prediction > 0.5).float()\n",
            "    # save prediction\n",
            "    prediction_filename = \"predictions/prediction_\" + str(i + 1) + \".png\"\n",
            "    prediction_filenames.append(prediction_filename)\n",
            "    plt.imsave(prediction_filename, prediction.squeeze().detach().numpy(), cmap=\"gray\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# create submission\n",
            "masks_to_submission(\"submission.csv\", *prediction_filenames)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Post Processing"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import cv2\n",
            "import numpy as np\n",
            "\n",
            "\n",
            "def apply_morphological_ops(segmentation_map):\n",
            "    # Define a kernel for operations (3x3 square)\n",
            "    kernel = np.ones((3, 3), np.uint8)\n",
            "\n",
            "    # Apply erosion\n",
            "    erosion = cv2.erode(segmentation_map, kernel, iterations=4)\n",
            "\n",
            "    # Apply dilation\n",
            "    dilation = cv2.dilate(erosion, kernel, iterations=4)\n",
            "\n",
            "    return dilation\n",
            "\n",
            "\n",
            "# Assuming 'segmentation_map' is your initial segmentation result as a binary image\n",
            "# processed_map = apply_morphological_ops(segmentation_map)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def postprocess_segmentation(image):\n",
            "    # Load the image (assuming it's a binary image with roads as white and background as black)\n",
            "\n",
            "    # Noise reduction (using morphological opening)\n",
            "    kernel_op = np.zeros((5, 5), np.uint8)\n",
            "    opening = cv2.morphologyEx(image, cv2.MORPH_OPEN, kernel_op, iterations=3)\n",
            "\n",
            "    closing = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, kernel_op, iterations=3)\n",
            "\n",
            "    # kernel_dil = np.zeros((3, 3), np.uint8)\n",
            "    # Background area determination (Dilation to increase the background area)\n",
            "    # sure_bg = cv2.dilate(opening, kernel_dil, iterations=3)\n",
            "\n",
            "    # # Identifying sure foreground area (roads)\n",
            "    # dist_transform = cv2.distanceTransform(opening, cv2.DIST_L1, 5)\n",
            "    # _, sure_fg = cv2.threshold(dist_transform, 0.7*dist_transform.max(), 255, 0)= 0\n",
            "\n",
            "    return closing"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "Python 3",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.8.18"
      },
      "vscode": {
         "interpreter": {
            "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
         }
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}