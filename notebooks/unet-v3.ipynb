{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "import segmentation_models_pytorch as smp\n",
    "from train_utils import prepare_data, prepare_model, prepare_optimizer, train, set_seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import yaml\n",
    "# import argparse\n",
    "# from omegaconf import OmegaConf\n",
    "# import wandb\n",
    "# import torch\n",
    "# import torchvision.transforms.v2 as transforms\n",
    "# from models.UNetV1 import UNetV1\n",
    "# from models.UNetV2 import UNetV2\n",
    "# from models.UNetV3 import UNetV3\n",
    "# from models.DeepLabV3 import ResNet101\n",
    "# from datasets.BaseDataset import BaseDataset\n",
    "# from datasets.TransformDataset import TransformDataset\n",
    "# import random\n",
    "\n",
    "\n",
    "# # set seeds as function\n",
    "# def set_seeds(seed=42):\n",
    "#     random.seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     torch.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed(seed)\n",
    "\n",
    "\n",
    "# def prepare_transforms(args):\n",
    "#     # prepare random transform (to be applied at the same time to both image and groundtruth, for consistency)\n",
    "#     # the goal is to avoid scenarios like flipping the image but not the groundtruth.\n",
    "#     random_transform = []\n",
    "#     # random resized crop\n",
    "#     if args.random_resized_crop:\n",
    "#         print(\n",
    "#             f\"Using RandomResizedCrop with output size={tuple(args.output_size)}, scale={tuple(args.random_resized_crop_scale)}.\"\n",
    "#         )\n",
    "#         random_transform.append(\n",
    "#             transforms.RandomResizedCrop(\n",
    "#                 size=tuple(args.output_size),\n",
    "#                 # scale=tuple(args.random_resized_crop_scale),\n",
    "#             )\n",
    "#         )\n",
    "#     # resize to 608x608, or pad 416x416\n",
    "#     # resize_transform = transforms.Pad(padding=8)\n",
    "#     resize_transform = transforms.Resize((384, 384))\n",
    "\n",
    "#     # random horizontal flip\n",
    "#     if args.random_horizontal_flip:\n",
    "#         print(\"Using RandomHorizontalFlip.\")\n",
    "#         random_transform.append(transforms.RandomHorizontalFlip())\n",
    "#     # random vertical flip\n",
    "#     if args.random_vertical_flip:\n",
    "#         print(\"Using RandomVerticalFlip.\")\n",
    "#         random_transform.append(transforms.RandomVerticalFlip())\n",
    "#     # random rotation\n",
    "#     if args.random_rotation:\n",
    "#         print(f\"Using RandomRotation with degrees={args.degrees}.\")\n",
    "#         random_transform.append(transforms.RandomRotation(degrees=args.degrees))\n",
    "#     # prepare additional transforms for image and groundtruth (these do not need to be applied at the same time)\n",
    "#     # the goal is to allow some flexibility in the transforms applied to the image and groundtruth, e.g. color jitter for image only.\n",
    "#     image_transform = []\n",
    "#     gt_transform = []\n",
    "#     # color jitter (for image only)\n",
    "#     if args.color_jitter:\n",
    "#         print(\n",
    "#             f\"Using ColorJitter with brightness={args.brightness}, contrast={args.contrast}, saturation={args.saturation}, hue={args.hue}.\"\n",
    "#         )\n",
    "#         image_transform.append(\n",
    "#             transforms.ColorJitter(\n",
    "#                 brightness=args.brightness,\n",
    "#                 contrast=args.contrast,\n",
    "#                 saturation=args.saturation,\n",
    "#                 hue=args.hue,\n",
    "#             )\n",
    "#         )\n",
    "#     # convert to tensors\n",
    "\n",
    "#     image_transform.append(transforms.ToTensor())\n",
    "#     gt_transform.append(transforms.ToTensor())\n",
    "#     image_transform.append(resize_transform)\n",
    "#     gt_transform.append(resize_transform)\n",
    "#     # normalization\n",
    "#     if args.normalization:\n",
    "#         std = [0.1967, 0.1896, 0.1897]\n",
    "#         means = [0.3353, 0.3328, 0.2984]\n",
    "#         # print(\"Using Normalize with mean=(0.5, 0.5, 0.5) and std=(0.5, 0.5, 0.5).\")\n",
    "#         image_transform.append(transforms.Normalize(mean=means, std=std))\n",
    "#         # gt_transform.append(transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))) -> No normalization for groundtruth\n",
    "#     # compose transforms\n",
    "#     # if there is no random transforms to be applied, set it to None\n",
    "#     if random_transform == []:\n",
    "#         random_transform = None\n",
    "#     else:\n",
    "#         random_transform = transforms.Compose(random_transform)\n",
    "#     image_transform = transforms.Compose(image_transform)\n",
    "#     gt_transform = transforms.Compose(gt_transform)\n",
    "#     # return (random_transform, image_transform, gt_transform)\n",
    "#     return random_transform, image_transform, gt_transform\n",
    "\n",
    "\n",
    "# def prepare_data(args):\n",
    "#     # get image and groundtruth transforms (for train set)\n",
    "#     random_transform, image_transform, gt_transform = prepare_transforms(args)\n",
    "#     # create transforms for images and groundtruths for validation and test sets\n",
    "#     # resize_transform = transforms.Pad(padding=8)\n",
    "#     resize_transform = transforms.Resize((384, 384))\n",
    "\n",
    "#     if args.normalization:\n",
    "#         std = [0.1967, 0.1896, 0.1897]\n",
    "#         means = [0.3353, 0.3328, 0.2984]\n",
    "#         tt_transform_image = transforms.Compose(\n",
    "#             [\n",
    "#                 resize_transform,\n",
    "#                 transforms.ToTensor(),\n",
    "#                 transforms.Normalize(means, std=std),\n",
    "#             ]\n",
    "#         )  # TODO: Should we always normalize?\n",
    "#     else:\n",
    "#         tt_transform_image = transforms.Compose(\n",
    "#             [resize_transform, transforms.ToTensor()]\n",
    "#         )\n",
    "\n",
    "#     tt_transform_gt = transforms.Compose(\n",
    "#         [resize_transform, transforms.ToTensor()]\n",
    "#     )  # No normalization for groundtruth!\n",
    "#     # create base dataset\n",
    "#     dataset = BaseDataset(image_folder=args.image_folder, gt_folder=args.gt_folder)\n",
    "#     # seed for reproducibility\n",
    "#     set_seeds()\n",
    "#     # split the dataset into train, validation and test set\n",
    "#     train_set, val_set, test_set = torch.utils.data.random_split(\n",
    "#         dataset,\n",
    "#         [\n",
    "#             int(args.train_size * len(dataset)),\n",
    "#             int(args.val_size * len(dataset)),\n",
    "#             int(args.test_size * len(dataset)),\n",
    "#         ],\n",
    "#     )\n",
    "#     # apply transforms\n",
    "#     train_set = TransformDataset(\n",
    "#         train_set,\n",
    "#         random_transform=random_transform,\n",
    "#         image_transform=image_transform,\n",
    "#         gt_transform=gt_transform,\n",
    "#     )\n",
    "#     val_set = TransformDataset(\n",
    "#         val_set,\n",
    "#         random_transform=None,\n",
    "#         image_transform=tt_transform_image,\n",
    "#         gt_transform=tt_transform_gt,\n",
    "#     )\n",
    "#     test_set = TransformDataset(\n",
    "#         test_set,\n",
    "#         random_transform=None,\n",
    "#         image_transform=tt_transform_image,\n",
    "#         gt_transform=tt_transform_gt,\n",
    "#     )\n",
    "#     # create data loaders\n",
    "#     train_loader = torch.utils.data.DataLoader(\n",
    "#         train_set, batch_size=args.batch_size, shuffle=True\n",
    "#     )\n",
    "#     val_loader = torch.utils.data.DataLoader(\n",
    "#         val_set, batch_size=args.batch_size, shuffle=True\n",
    "#     )\n",
    "#     test_loader = torch.utils.data.DataLoader(\n",
    "#         test_set, batch_size=args.batch_size, shuffle=True\n",
    "#     )\n",
    "#     # return data loaders\n",
    "#     return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "# def prepare_model(args):\n",
    "#     if args.model_name == \"UNetV1\":\n",
    "#         print(\n",
    "#             f\"Initializing UNetV1 model with pretrained={args.model_pretrained}, scale={args.model_scale}.\"\n",
    "#         )\n",
    "#         model = UNetV1(pretrained=args.model_pretrained, scale=args.model_scale)\n",
    "#     elif args.model_name == \"UNetV2\":\n",
    "#         print(\n",
    "#             f\"Initializing UNetV2 model with in_channels={args.model_in_channels}, out_channels={args.model_out_channels}, init_features={args.model_init_features}, pretrained={args.model_pretrained}.\"\n",
    "#         )\n",
    "#         model = UNetV2(\n",
    "#             in_channels=args.model_in_channels,\n",
    "#             out_channels=args.model_out_channels,\n",
    "#             init_features=args.model_init_features,\n",
    "#             pretrained=args.model_pretrained,\n",
    "#         )\n",
    "#     elif args.model_name == \"UNetV3\":\n",
    "#         print(\"Initializing UNetV3 model.\")\n",
    "#         model = UNetV3()\n",
    "#     elif args.model_name == \"ResNet101\":\n",
    "#         print(\"Initializing ResNet101 model.\")\n",
    "#         model = ResNet101()\n",
    "#     ## ADD MODELS HERE!\n",
    "#     return model\n",
    "\n",
    "\n",
    "# def prepare_optimizer(model, args):\n",
    "#     if args.optim_name == \"sgd\":\n",
    "#         print(\n",
    "#             f\"Initializing SGD optimizer with lr={args.optim_lr}, momentum={args.optim_momentum}.\"\n",
    "#         )\n",
    "#         optimizer = torch.optim.SGD(\n",
    "#             model.parameters(), lr=args.optim_lr, momentum=args.optim_momentum\n",
    "#         )\n",
    "#     elif args.optim_name == \"adam\":\n",
    "#         print(f\"Initializing Adam optimizer with lr={args.optim_lr}.\")\n",
    "#         optimizer = torch.optim.Adam(model.parameters(), lr=args.optim_lr)\n",
    "#     return optimizer\n",
    "\n",
    "\n",
    "# def step_loader(loader, n_steps=-1):\n",
    "#     # creates infinite loader for training (\n",
    "#     # use as metric n_steps instead of n_epochs\n",
    "#     step = 0\n",
    "#     while True:\n",
    "#         for batch in loader:\n",
    "#             yield step, batch\n",
    "#             step += 1\n",
    "#             if step == n_steps:\n",
    "#                 return\n",
    "\n",
    "\n",
    "# def calculate_metrics(preds, labels):\n",
    "#     # torch.Size([2, 1, 416, 416])\n",
    "#     # torch.Size([2, 1, 416, 416])\n",
    "\n",
    "#     smooth = 1e-6\n",
    "#     # convert predictions to binary format\n",
    "#     preds = torch.sigmoid(preds)\n",
    "#     preds = (preds > 0.5).float()\n",
    "\n",
    "#     # pixel Accuracy\n",
    "#     pixel_accuracy = (preds == labels).sum().item() / (labels.numel() + smooth)\n",
    "\n",
    "#     # Intersection-Over-Union (IoU)\n",
    "#     intersection = (preds * labels).sum()\n",
    "#     union = preds.sum() + labels.sum() - intersection\n",
    "#     iou = (intersection + smooth) / (union + smooth)\n",
    "#     return pixel_accuracy, iou.item()\n",
    "\n",
    "\n",
    "# def train(model, device, train_loader, val_loader, criterion, optimizer, args):\n",
    "#     # set up WandB for logging\n",
    "#     config_dict = OmegaConf.to_container(args, resolve=True)\n",
    "#     wandb.init(\n",
    "#         project=args.wandb_project,\n",
    "#         name=args.wandb_run,\n",
    "#         config=config_dict,\n",
    "#         entity=args.entity,\n",
    "#     )\n",
    "#     # Upload the configuration file to WandB\n",
    "#     wandb.config.update(config_dict)\n",
    "#     best_iou_score = 0.0\n",
    "#     # training loop\n",
    "#     for step, batch in step_loader(train_loader, args.n_steps):\n",
    "#         # training\n",
    "#         model.train()\n",
    "#         inputs, labels = batch\n",
    "#         inputs, labels = inputs.to(device), labels.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         # logging\n",
    "#         wandb.log({\"Training Loss\": loss.item()}, step=step)\n",
    "#         train_pixel_accuracy, train_iou = calculate_metrics(outputs, labels)\n",
    "#         wandb.log(\n",
    "#             {\n",
    "#                 \"Training Pixel Accuracy\": train_pixel_accuracy,\n",
    "#                 \"Training IoU\": train_iou,\n",
    "#             },\n",
    "#             step=step,\n",
    "#         )\n",
    "#         # TODO\n",
    "#         # validation\n",
    "#         if step % args.eval_freq == 0:\n",
    "#             model.eval()\n",
    "#             total_val_loss = 0.0\n",
    "#             total_pixel_accuracy = 0.0\n",
    "#             total_iou = 0.0\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 for val_inputs, val_targets in val_loader:\n",
    "#                     val_inputs = val_inputs.to(device)\n",
    "#                     val_targets = val_targets.to(device)\n",
    "\n",
    "#                     val_outputs = model(val_inputs)\n",
    "#                     val_loss = criterion(val_outputs, val_targets)\n",
    "#                     total_val_loss += val_loss.item()\n",
    "\n",
    "#                     # calculate metrics for validation data\n",
    "#                     val_pixel_accuracy, val_iou = calculate_metrics(\n",
    "#                         val_outputs, val_targets\n",
    "#                     )\n",
    "#                     total_pixel_accuracy += val_pixel_accuracy\n",
    "#                     total_iou += val_iou\n",
    "\n",
    "#             avg_pixel_accuracy = total_pixel_accuracy / len(val_loader)\n",
    "#             avg_val_loss = total_val_loss / len(val_loader)\n",
    "#             avg_iou = total_iou / len(val_loader)\n",
    "#             wandb.log(\n",
    "#                 {\n",
    "#                     \"Average Validation Loss\": avg_val_loss,\n",
    "#                     \"Average Validation Pixel Accuracy\": avg_pixel_accuracy,\n",
    "#                     \"Average Validation IoU\": avg_iou,\n",
    "#                 },\n",
    "#                 step=step,\n",
    "#             )\n",
    "#             # Save the model if this is the best F1 score so far\n",
    "#             if avg_iou > best_iou_score:\n",
    "#                 best_iou_score = avg_iou\n",
    "#                 torch.save(model.state_dict(), args.model_save_name)\n",
    "#                 print(\"Best model saved at step: \", step)\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seeds\n",
    "set_seeds()\n",
    "\n",
    "# create folder models if it doesn't exist\n",
    "if not os.path.exists(\"models\"):\n",
    "    os.makedirs(\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = OmegaConf.create(\n",
    "    dict(\n",
    "        # General\n",
    "        seed=0,\n",
    "        # Data folders\n",
    "        image_folder=\"datasets/train/images/\",\n",
    "        gt_folder=\"datasets/train/groundtruth/\",\n",
    "        # Data tranforms\n",
    "        # random_resized_crop: crop a random portion of image and resize it to a given size\n",
    "        random_resized_crop=False,\n",
    "        output_size=(400, 400),  # expected output size of the crop, for each edge.\n",
    "        input_size=(384),  # resize\n",
    "        random_resized_crop_scale=(0.5, 0.5),\n",
    "        # random_horizontal_flip: randomly flip the image horizontally with a given probability\n",
    "        random_horizontal_flip=True,\n",
    "        # random_vertical_flip: randomly flip the image vertically with a given probability\n",
    "        random_vertical_flip=True,\n",
    "        # random_rotation: randomly rotate the image with a given probability\n",
    "        random_rotation=False,\n",
    "        degrees=5,  # range of degrees to select from\n",
    "        # color_jitter: randomly change the brightness, contrast and saturation of an image\n",
    "        color_jitter=False,\n",
    "        brightness=0.1,  # how much to jitter brightness.\n",
    "        contrast=0.1,  # how much to jitter contrast.\n",
    "        saturation=0.1,  # how much to jitter saturation.\n",
    "        hue=0.1,  # how much to jitter hue.\n",
    "        # normalization\n",
    "        normalization=True,  # TODO: Should it always be True?\n",
    "        # Data loaders\n",
    "        batch_size=2,\n",
    "        train_size=0.8,\n",
    "        val_size=0.1,\n",
    "        test_size=0.1,\n",
    "        # Model\n",
    "        # UNetV1\n",
    "        model_name=\"UNetV3\",  # Change for v3\n",
    "        model_save_name=\"models/checkpoints/unetv3-4.pt\",\n",
    "        # Optimizer\n",
    "        optim_name=\"adam\",  # sgd\n",
    "        optim_lr=0.001,\n",
    "        optim_momentum=0.9,  # TODO: Try with (optim_momentum != 0) and without (momentum = 0)?\n",
    "        # Training\n",
    "        n_steps=2000,\n",
    "        eval_freq=100,\n",
    "        # Wandb logging\n",
    "        wandb_project=\"road-segmentation\",\n",
    "        wandb_run=\"unet-v3\",\n",
    "        entity=\"feeit\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare train, validation and test loaders\n",
    "train_loader, val_loader, test_loader = prepare_data(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "# criterion = DiceLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare optimizer\n",
    "optimizer = prepare_optimizer(model, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device to use for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "trained_model = train(\n",
    "    model, device, train_loader, val_loader, criterion, optimizer, args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from train_utils import batch_mean_and_sd\n",
    "\n",
    "# # calculate mean and std of the dataset\n",
    "# mean, std = batch_mean_and_sd(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transforms\n",
    "from torchvision import transforms\n",
    "from datasets.TestDataset import TestDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from examples.mask_to_submission import *\n",
    "import torchvision.models.segmentation as models\n",
    "from models.UNetV3 import UNetV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create segmentation model with pretrained encoder\n",
    "model = UNetV3()\n",
    "MODEL = \"models/checkpoints/unetv3-3_effnet.pt\"\n",
    "checkpoint = torch.load(MODEL)\n",
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the test folder\n",
    "test_folder = \"datasets/test/\"\n",
    "std = [0.1967, 0.1896, 0.1897]\n",
    "means = [0.3353, 0.3328, 0.2984]\n",
    "\n",
    "# define transformations\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(means, std=std),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# create test dataset\n",
    "test_dataset = TestDataset(test_folder, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predictions\n",
    "prediction_filenames = []\n",
    "for i in range(len(test_dataset)):\n",
    "    # get image\n",
    "    image = test_dataset[i]\n",
    "    # create prediction\n",
    "    prediction = model(image.unsqueeze(0))\n",
    "    # threshold prediction\n",
    "    prediction = torch.sigmoid(prediction)\n",
    "    prediction = (prediction > 0.5).float()\n",
    "    # save prediction\n",
    "    prediction_filename = \"predictions/prediction_\" + str(i + 1) + \".png\"\n",
    "    prediction_filenames.append(prediction_filename)\n",
    "    plt.imsave(prediction_filename, prediction.squeeze().detach().numpy(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create submission\n",
    "masks_to_submission(\"submission.csv\", *prediction_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def apply_morphological_ops(segmentation_map):\n",
    "    # Define a kernel for operations (3x3 square)\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "\n",
    "    # Apply erosion\n",
    "    erosion = cv2.erode(segmentation_map, kernel, iterations=4)\n",
    "\n",
    "    # Apply dilation\n",
    "    dilation = cv2.dilate(erosion, kernel, iterations=4)\n",
    "\n",
    "    return dilation\n",
    "\n",
    "\n",
    "# Assuming 'segmentation_map' is your initial segmentation result as a binary image\n",
    "# processed_map = apply_morphological_ops(segmentation_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_segmentation(image):\n",
    "    # Load the image (assuming it's a binary image with roads as white and background as black)\n",
    "\n",
    "    # Noise reduction (using morphological opening)\n",
    "    kernel_op = np.zeros((5, 5), np.uint8)\n",
    "    opening = cv2.morphologyEx(image, cv2.MORPH_OPEN, kernel_op, iterations=3)\n",
    "\n",
    "    closing = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, kernel_op, iterations=3)\n",
    "\n",
    "    # kernel_dil = np.zeros((3, 3), np.uint8)\n",
    "    # Background area determination (Dilation to increase the background area)\n",
    "    # sure_bg = cv2.dilate(opening, kernel_dil, iterations=3)\n",
    "\n",
    "    # # Identifying sure foreground area (roads)\n",
    "    # dist_transform = cv2.distanceTransform(opening, cv2.DIST_L1, 5)\n",
    "    # _, sure_fg = cv2.threshold(dist_transform, 0.7*dist_transform.max(), 255, 0)= 0\n",
    "\n",
    "    return closing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "road_segmentation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
